{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing your Parquet files\n",
    "directory_path = 'C:/Users/comma/Github/workspace/sas-access-samples/SAS Foundation/parquetdataset/'\n",
    "\n",
    "# Use glob to find all Parquet files in the directory\n",
    "parquet_file_paths = glob.glob(directory_path + '*.parquet')\n",
    "\n",
    "# Initialize an empty dictoionary to store the DataFrames\n",
    "dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode bytes to string if the value is a byte literal\n",
    "def decode_bytes(val):\n",
    "    if isinstance(val, bytes):\n",
    "        return val.decode('utf-8')\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the file paths\n",
    "for path in parquet_file_paths:\n",
    "    # Extract the base file name without the extension as the dictionary key\n",
    "    file_name = path.split('/')[-1].replace('.parquet', '')\n",
    "    \n",
    "    # Read each file into a DataFrame\n",
    "    df = pd.read_parquet(path)\n",
    "    \n",
    "    # Apply the decode_bytes function to each column using Series.map\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].map(decode_bytes)\n",
    "    \n",
    "    # Store the DataFrame in the dictionary with the file name as the key\n",
    "    dfs[file_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfs['parquetdataset\\\\samdat1']\n",
    "df2 = dfs['parquetdataset\\\\samdat2']\n",
    "df3 = dfs['parquetdataset\\\\samdat3']\n",
    "df4 = dfs['parquetdataset\\\\samdat4']\n",
    "df5 = dfs['parquetdataset\\\\samdat5']\n",
    "df6 = dfs['parquetdataset\\\\samdat6']\n",
    "df7 = dfs['parquetdataset\\\\samdat7']\n",
    "df8 = dfs['parquetdataset\\\\samdat8']\n",
    "df9 = dfs['parquetdataset\\\\samdat9']\n",
    "df10 = dfs['parquetdataset\\\\samdat10']\n",
    "df11 = dfs['parquetdataset\\\\samdat11']\n",
    "df12 = dfs['parquetdataset\\\\samdat12']\n",
    "df13 = dfs['parquetdataset\\\\samdat13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   DEPART    46 non-null     float64       \n",
      " 3   ORIG      46 non-null     object        \n",
      " 4   DEST      46 non-null     object        \n",
      " 5   MILES     46 non-null     float64       \n",
      " 6   BOARDED   46 non-null     float64       \n",
      " 7   CAPACITY  46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(4), object(3)\n",
      "memory usage: 3.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   ORIG      46 non-null     object        \n",
      " 3   DEST      46 non-null     object        \n",
      " 4   DELAYCAT  46 non-null     object        \n",
      " 5   DESTYPE   46 non-null     object        \n",
      " 6   DELAY     46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(5)\n",
      "memory usage: 2.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26 entries, 0 to 25\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   FLIGHT   26 non-null     object        \n",
      " 1   DATES    26 non-null     datetime64[ns]\n",
      " 2   DEST     26 non-null     object        \n",
      " 3   BOARDED  26 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(2)\n",
      "memory usage: 964.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   FLIGHT  156 non-null    object        \n",
      " 1   DATES   156 non-null    datetime64[ns]\n",
      " 2   DEST    156 non-null    object        \n",
      " 3   IDNUM   156 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 5.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148 entries, 0 to 147\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   IDNUM    148 non-null    object        \n",
      " 1   SEX      148 non-null    object        \n",
      " 2   JOBCODE  148 non-null    object        \n",
      " 3   SALARY   148 non-null    float64       \n",
      " 4   BIRTH    148 non-null    datetime64[ns]\n",
      " 5   HIRED    148 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(1), object(3)\n",
      "memory usage: 7.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12 entries, 0 to 11\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   IDNUM    12 non-null     object        \n",
      " 1   SEX      12 non-null     object        \n",
      " 2   JOBCODE  12 non-null     object        \n",
      " 3   SALARY   12 non-null     float64       \n",
      " 4   BIRTH    12 non-null     datetime64[ns]\n",
      " 5   HIRED    12 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(1), object(3)\n",
      "memory usage: 708.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148 entries, 0 to 147\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   IDNUM   148 non-null    object\n",
      " 1   LNAME   148 non-null    object\n",
      " 2   FNAME   148 non-null    object\n",
      " 3   CITY    148 non-null    object\n",
      " 4   STATE   148 non-null    object\n",
      " 5   HPHONE  148 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 7.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   SUPID   19 non-null     object\n",
      " 1   STATE   19 non-null     object\n",
      " 2   JOBCAT  19 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 588.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17 entries, 0 to 16\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   INVNUM    17 non-null     float64       \n",
      " 1   BILLEDTO  17 non-null     object        \n",
      " 2   AMTBILL   17 non-null     float64       \n",
      " 3   COUNTRY   17 non-null     object        \n",
      " 4   AMTINUS   17 non-null     float64       \n",
      " 5   BILLEDBY  17 non-null     float64       \n",
      " 6   BILLEDON  17 non-null     datetime64[ns]\n",
      " 7   PAIDON    10 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(4), object(2)\n",
      "memory usage: 1.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   EMPID     22 non-null     float64       \n",
      " 1   HIREDATE  22 non-null     datetime64[ns]\n",
      " 2   SALARY    21 non-null     float64       \n",
      " 3   DEPT      22 non-null     object        \n",
      " 4   JOBCODE   22 non-null     float64       \n",
      " 5   GENDER    21 non-null     object        \n",
      " 6   BIRTHDTE  21 non-null     datetime64[ns]\n",
      " 7   LASTNAME  22 non-null     object        \n",
      " 8   FRSTNAME  22 non-null     object        \n",
      " 9   MIDNAME   22 non-null     object        \n",
      " 10  PHONE     21 non-null     object        \n",
      "dtypes: datetime64[ns](2), float64(3), object(6)\n",
      "memory usage: 2.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   CUSTOMER  21 non-null     object        \n",
      " 1   STATE     11 non-null     object        \n",
      " 2   ZIPCODE   15 non-null     object        \n",
      " 3   COUNTRY   20 non-null     object        \n",
      " 4   PHONE     21 non-null     object        \n",
      " 5   NAME      20 non-null     object        \n",
      " 6   CONTACT   19 non-null     object        \n",
      " 7   ADDRESS   20 non-null     object        \n",
      " 8   CITY      20 non-null     object        \n",
      " 9   FIRSTORD  21 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(9)\n",
      "memory usage: 1.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38 entries, 0 to 37\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   ORDERNUM  38 non-null     float64       \n",
      " 1   STOCKNUM  38 non-null     float64       \n",
      " 2   LENGTH    38 non-null     float64       \n",
      " 3   FABCHARG  24 non-null     float64       \n",
      " 4   SHIPTO    38 non-null     object        \n",
      " 5   DATEORD   38 non-null     datetime64[ns]\n",
      " 6   SHIPPED   15 non-null     datetime64[ns]\n",
      " 7   TAKENBY   38 non-null     float64       \n",
      " 8   PROCSBY   15 non-null     float64       \n",
      " 9   SPECFLAG  10 non-null     object        \n",
      "dtypes: datetime64[ns](2), float64(6), object(2)\n",
      "memory usage: 3.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   EMPID     6 non-null      float64       \n",
      " 1   HIREDATE  6 non-null      datetime64[ns]\n",
      " 2   DEPT      6 non-null      object        \n",
      " 3   GENDER    5 non-null      object        \n",
      " 4   LASTNAME  6 non-null      object        \n",
      " 5   FIRSTNAM  6 non-null      object        \n",
      " 6   MIDDLENA  5 non-null      object        \n",
      " 7   FAMILYID  2 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(5)\n",
      "memory usage: 516.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df1.info()\n",
    "df2.info()\n",
    "df3.info()\n",
    "df4.info()\n",
    "df5.info()\n",
    "df6.info()\n",
    "df7.info()\n",
    "df8.info()\n",
    "df9.info()\n",
    "df10.info()\n",
    "df11.info()\n",
    "df12.info()\n",
    "df13.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 1       */\n",
    " /*=========================*/\n",
    "\n",
    "proc print data=samples.SAMDAT7\n",
    "   (keep=lname fname state hphone);\n",
    "   where state = 'NJ';\n",
    "   title 'Libname Sample 1: New Jersey Phone List';\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code aims to filter a dataset named 'SAMDAT7' (located in the 'samples' library) for records where the state is 'NJ'. It then prints a selected subset of columns from the filtered data with the title 'Libname Sample 1: New Jersey Phone List'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         LNAME    FNAME STATE        HPHONE\n",
      "17      RHODES   JEREMY    NJ  201/812-1837\n",
      "62     ALVAREZ   CARLOS    NJ  201/732-8787\n",
      "66       DACKO    JASON    NJ  201/732-2323\n",
      "69   HENDERSON  WILLIAM    NJ  201/812-4789\n",
      "72     JOHNSON  JACKSON    NJ  201/732-3678\n",
      "73     MURPHEY     JOHN    NJ  201/812-4414\n",
      "74      PETERS  RANDALL    NJ  201/812-2478\n",
      "77     NEWKIRK  WILLIAM    NJ  201/732-6611\n",
      "79       ROUSE   JEREMY    NJ  201/732-9834\n",
      "81    FUJIHARA    KYOKO    NJ  201/812-0902\n",
      "85        VICK  THERESA    NJ  201/812-2424\n",
      "92      YANCEY    ROBIN    NJ  201/812-1874\n",
      "100   LAWRENCE    KATHY    NJ  201/812-3337\n",
      "101    NEWKIRK   SANDRA    NJ  201/812-3331\n",
      "109   BAREFOOT   JOSEPH    NJ  201/812-5665\n",
      "Number of rows: 15\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Sample 1: New Jersey Phone List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have df7 loaded from your input mapping\n",
    "df_filtered = df7[df7['STATE'] == 'NJ']  # Filtering for New Jersey records\n",
    "\n",
    "# Select and rename columns (all to uppercase)\n",
    "df_output = df_filtered[['LNAME', 'FNAME', 'STATE', 'HPHONE']]\n",
    "df_output.columns = ['LNAME', 'FNAME', 'STATE', 'HPHONE'] \n",
    "\n",
    "# Display Result\n",
    "print(df_output)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_output.shape[0])\n",
    "print(\"Number of columns:\", df_output.shape[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*=========================*/\n",
    " /* LIBNAME Sample 2       */\n",
    " /*=========================*/\n",
    "\n",
    "data work.highwage;\n",
    "  set samples.SAMDAT5(drop=sex birth hired);\n",
    "  if salary>60000 then\n",
    "    CATEGORY='High';\n",
    "  else if salary<30000 then\n",
    "    CATEGORY='Low';\n",
    "  else\n",
    "    CATEGORY='Avg';\n",
    "run;\n",
    "\n",
    "proc print data=work.highwage;\n",
    "  title 'Libname Sample 2: Salary Analysis';\n",
    "  format SALARY dollar10.2;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Creates a new dataset: It creates a dataset named 'highwage' in the 'work' library.\n",
    "Data Derivation: This dataset is derived from the 'SAMDAT5' dataset (in the 'samples' library), but it drops the 'sex', 'birth', and 'hired' columns.\n",
    "Conditional Logic: It adds a new column named 'CATEGORY' and assigns values ('High', 'Low', or 'Avg') based on the value in the 'SALARY' column.\n",
    "Prints Results: It prints the 'highwage' dataset with a specific format for the 'SALARY' column and a custom title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDNUM JOBCODE      SALARY CATEGORY\n",
      "0    1009     TA1  $28,880.00      Low\n",
      "1    1017     TA3  $40,858.00      Avg\n",
      "2    1036     TA3  $39,392.00      Avg\n",
      "3    1037     TA1  $28,558.00      Low\n",
      "4    1038     TA1  $26,533.00      Low\n",
      "5    1050     ME2  $35,167.00      Avg\n",
      "6    1065     ME2  $35,090.00      Avg\n",
      "7    1076     PT1  $66,558.00     High\n",
      "8    1094     FA1  $22,268.00      Low\n",
      "9    1100     BCK  $25,004.00      Low\n",
      "10   1101     SCP  $18,723.00      Low\n",
      "11   1102     TA2  $34,542.00      Avg\n",
      "12   1103     FA1  $23,738.00      Low\n",
      "13   1104     SCP  $17,946.00      Low\n",
      "14   1105     ME2  $34,805.00      Avg\n",
      "15   1106     PT2  $89,632.00     High\n",
      "16   1107     PT2  $89,977.00     High\n",
      "17   1111     NA1  $40,586.00      Avg\n",
      "18   1112     TA1  $26,905.00      Low\n",
      "19   1113     FA1  $22,367.00      Low\n",
      "20   1114     TA2  $32,928.00      Avg\n",
      "21   1115     FA3  $32,699.00      Avg\n",
      "22   1116     FA1  $22,862.00      Low\n",
      "23   1117     TA3  $39,771.00      Avg\n",
      "24   1118     PT3 $111,379.00     High\n",
      "25   1119     TA1  $26,924.00      Low\n",
      "26   1120     ME1  $28,619.00      Low\n",
      "27   1121     ME1  $29,112.00      Low\n",
      "28   1122     FA2  $27,956.00      Low\n",
      "29   1123     TA1  $28,407.00      Low\n",
      "30   1124     FA1  $23,177.00      Low\n",
      "31   1125     FA2  $28,888.00      Low\n",
      "32   1126     TA3  $40,899.00      Avg\n",
      "33   1127     TA2  $33,011.00      Avg\n",
      "34   1128     TA2  $32,777.00      Avg\n",
      "35   1129     ME2  $34,929.00      Avg\n",
      "36   1130     FA1  $23,916.00      Low\n",
      "37   1131     TA2  $32,575.00      Avg\n",
      "38   1132     FA1  $22,413.00      Low\n",
      "39   1133     TA1  $27,701.00      Low\n",
      "40   1134     TA2  $33,462.00      Avg\n",
      "41   1135     FA2  $27,321.00      Low\n",
      "42   1200     ME1  $27,816.00      Low\n",
      "43   1221     FA2  $27,896.00      Low\n",
      "44   1244     ME2  $36,925.00      Avg\n",
      "45   1269     NA1  $41,690.00      Avg\n",
      "46   1292     ME2  $36,691.00      Avg\n",
      "47   1332     NA1  $42,178.00      Avg\n",
      "48   1333     PT2  $88,606.00     High\n",
      "49   1347     TA3  $40,079.00      Avg\n",
      "50   1350     FA3  $32,886.00      Avg\n",
      "51   1352     NA2  $53,798.00      Avg\n",
      "52   1354     SCP  $18,335.00      Low\n",
      "53   1356     ME2  $36,869.00      Avg\n",
      "54   1368     FA2  $27,808.00      Low\n",
      "55   1369     TA2  $33,705.00      Avg\n",
      "56   1379     ME3  $42,264.00      Avg\n",
      "57   1383     BCK  $25,823.00      Low\n",
      "58   1385     ME3  $43,900.00      Avg\n",
      "59   1389     BCK  $25,028.00      Low\n",
      "60   1390     FA2  $27,761.00      Low\n",
      "61   1400     ME1  $29,769.00      Low\n",
      "62   1401     TA3  $38,822.00      Avg\n",
      "63   1402     TA2  $32,615.00      Avg\n",
      "64   1403     ME1  $28,072.00      Low\n",
      "65   1404     PT2  $91,376.00     High\n",
      "66   1405     SCP  $18,056.00      Low\n",
      "67   1406     ME2  $35,185.00      Avg\n",
      "68   1407     PT1  $68,096.00     High\n",
      "69   1408     TA2  $34,138.00      Avg\n",
      "70   1409     ME3  $41,551.00      Avg\n",
      "71   1410     PT2  $84,685.00     High\n",
      "72   1411     FA2  $27,265.00      Low\n",
      "73   1412     ME1  $27,799.00      Low\n",
      "74   1413     FA2  $27,435.00      Low\n",
      "75   1414     FA1  $23,644.00      Low\n",
      "76   1415     FA2  $28,278.00      Low\n",
      "77   1417     NA2  $52,270.00      Avg\n",
      "78   1418     ME1  $28,005.00      Low\n",
      "79   1420     ME3  $43,071.00      Avg\n",
      "80   1421     TA2  $33,155.00      Avg\n",
      "81   1422     FA1  $22,454.00      Low\n",
      "82   1423     ME2  $35,773.00      Avg\n",
      "83   1424     FA2  $28,978.00      Low\n",
      "84   1425     FA1  $23,979.00      Low\n",
      "85   1426     TA2  $32,991.00      Avg\n",
      "86   1427     TA2  $34,046.00      Avg\n",
      "87   1428     PT1  $68,767.00     High\n",
      "88   1429     TA1  $27,939.00      Low\n",
      "89   1430     TA2  $32,925.00      Avg\n",
      "90   1431     FA3  $33,230.00      Avg\n",
      "91   1432     ME2  $35,327.00      Avg\n",
      "92   1433     FA3  $32,982.00      Avg\n",
      "93   1434     FA2  $28,622.00      Low\n",
      "94   1435     TA3  $38,808.00      Avg\n",
      "95   1436     TA2  $34,475.00      Avg\n",
      "96   1437     FA3  $33,104.00      Avg\n",
      "97   1438     TA3  $39,223.00      Avg\n",
      "98   1439     PT1  $70,736.00     High\n",
      "99   1440     ME2  $35,757.00      Avg\n",
      "100  1441     FA2  $27,158.00      Low\n",
      "101  1442     PT2  $84,536.00     High\n",
      "102  1443     NA1  $42,274.00      Avg\n",
      "103  1475     FA2  $27,787.00      Low\n",
      "104  1476     TA2  $34,803.00      Avg\n",
      "105  1477     FA2  $28,566.00      Low\n",
      "106  1478     PT2  $84,203.00     High\n",
      "107  1479     TA3  $38,785.00      Avg\n",
      "108  1480     TA3  $39,583.00      Avg\n",
      "109  1499     ME3  $43,025.00      Avg\n",
      "110  1521     ME3  $41,526.00      Avg\n",
      "111  1545     PT1  $66,130.00     High\n",
      "112  1555     FA2  $27,499.00      Low\n",
      "113  1556     PT1  $71,349.00     High\n",
      "114  1561     TA2  $34,514.00      Avg\n",
      "115  1564     SCP  $18,833.00      Low\n",
      "116  1574     FA2  $28,572.00      Low\n",
      "117  1616     TA2  $34,137.00      Avg\n",
      "118  1639     TA3  $40,260.00      Avg\n",
      "119  1653     ME2  $35,108.00      Avg\n",
      "120  1658     SCP  $17,943.00      Low\n",
      "121  1663     BCK  $26,452.00      Low\n",
      "122  1673     BCK  $25,477.00      Low\n",
      "123  1677     BCK  $26,007.00      Low\n",
      "124  1704     BCK  $25,465.00      Low\n",
      "125  1739     PT1  $66,517.00     High\n",
      "126  1777     PT3 $109,630.00     High\n",
      "127  1782     ME2  $35,345.00      Avg\n",
      "128  1789     SCP  $18,326.00      Low\n",
      "129  1830     PT2  $84,471.00     High\n",
      "130  1834     BCK  $26,896.00      Low\n",
      "131  1839     NA1  $43,433.00      Avg\n",
      "132  1845     BCK  $25,996.00      Low\n",
      "133  1876     TA3  $39,675.00      Avg\n",
      "134  1882     ME3  $41,538.00      Avg\n",
      "135  1890     PT2  $91,908.00     High\n",
      "136  1900     ME2  $35,105.00      Avg\n",
      "137  1905     PT1  $65,111.00     High\n",
      "138  1907     TA2  $33,329.00      Avg\n",
      "139  1908     TA2  $32,995.00      Avg\n",
      "140  1919     TA2  $34,376.00      Avg\n",
      "141  1928     PT2  $89,858.00     High\n",
      "142  1935     NA2  $51,081.00      Avg\n",
      "143  1970     FA1  $22,615.00      Low\n",
      "144  1983     FA3  $33,419.00      Avg\n",
      "145  1988     FA3  $32,217.00      Avg\n",
      "146  1991     TA1  $27,645.00      Low\n",
      "147  1995     ME1  $28,810.00      Low\n",
      "Number of rows: 148\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Sample 2: Salary Analysis\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have df5 loaded from your input mapping\n",
    "\n",
    "# Create a new DataFrame \n",
    "df_highwage = df5.copy()  #  Make a copy to avoid modifying the original\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_highwage.drop(['SEX', 'BIRTH', 'HIRED'], axis=1, inplace=True)\n",
    "\n",
    "# Conditional logic for 'CATEGORY' column\n",
    "def categorize_salary(salary):\n",
    "    if salary > 60000:\n",
    "        return 'High'\n",
    "    elif salary < 30000:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Avg'\n",
    "\n",
    "df_highwage['CATEGORY'] = df_highwage['SALARY'].apply(categorize_salary)\n",
    "\n",
    "# Display Result (with formatting)\n",
    "print(df_highwage.to_string(formatters={'SALARY': '${:,.2f}'.format}))  \n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_highwage.shape[0])\n",
    "print(\"Number of columns:\", df_highwage.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*=========================*/\n",
    " /* LIBNAME Sample 3       */\n",
    " /*=========================*/\n",
    "\n",
    " data work.combined;\n",
    "  merge samples.SAMDAT7 samples.SAMDAT8(in=super\n",
    "    rename=(SUPID=IDNUM));\n",
    "  by IDNUM;\n",
    "  if super;\n",
    "run;\n",
    "\n",
    "proc print data=work.combined;\n",
    "  title 'Libname Sample 3: Supervisor Information';\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SAS code does the following:\n",
    "\n",
    "Merges Datasets: It merges two datasets, 'SAMDAT7' and 'SAMDAT8' (both in the 'samples' library), into a new dataset named 'combined' in the 'work' library.\n",
    "Join Condition: The merge is performed using the 'IDNUM' column as the common key for joining.\n",
    "Renaming: The 'SUPID' column from 'SAMDAT8' is renamed to 'IDNUM' during the merge.\n",
    "Filtering: The if super; statement filters the merged dataset to include only rows where a variable named 'super' has a truthy value (likely indicating supervisor status).\n",
    "Prints Results: Prints the 'combined' dataset with a custom title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDNUM      LNAME      FNAME        CITY STATE        HPHONE\n",
      "0    1009     MORGAN     GEORGE    NEW YORK    NY  212/586-7753\n",
      "1    1017      WELCH     DARIUS    NEW YORK    NY  212/586-5535\n",
      "2    1036       WONG     LESLIE    NEW YORK    NY  212/587-2570\n",
      "3    1037       CHOW       JANE    STAMFORD    CT  203/781-8868\n",
      "4    1038  RODRIGUEZ      MARIA  BRIDGEPORT    CT  203/675-2048\n",
      "..    ...        ...        ...         ...   ...           ...\n",
      "143  1970     PARKER       ANNE    NEW YORK    NY  718/383-3895\n",
      "144  1983       DEAN     SHARON    NEW YORK    NY  718/384-1647\n",
      "145  1988     COOPER    ANTHONY    NEW YORK    NY  212/587-1228\n",
      "146  1991     HOWARD   GRETCHEN  BRIDGEPORT    CT  203/675-0007\n",
      "147  1995     VARNER  ELIZABETH    NEW YORK    NY  718/384-7113\n",
      "\n",
      "[148 rows x 6 columns]\n",
      "   SUPID STATE JOBCAT\n",
      "0   1106    CT     PT\n",
      "1   1118    NY     PT\n",
      "2   1126    NY     TA\n",
      "3   1352    NY     NA\n",
      "4   1385    CT     ME\n",
      "5   1401    NJ     TA\n",
      "6   1405    NJ     SC\n",
      "7   1417    NJ     NA\n",
      "8   1420    NJ     ME\n",
      "9   1431    CT     FA\n",
      "10  1433    NJ     FA\n",
      "11  1442    NJ     PT\n",
      "12  1564    NY     SC\n",
      "13  1639    CT     TA\n",
      "14  1677    CT     BC\n",
      "15  1834    NY     BC\n",
      "16  1882    NY     ME\n",
      "17  1935    CT     NA\n",
      "18  1983    NY     FA\n"
     ]
    }
   ],
   "source": [
    "print(df7)\n",
    "print(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'super' (or a case variant) not found. Filtering skipped.\n",
      "    IDNUM         LNAME      FNAME          CITY STATE        HPHONE SEX  \\\n",
      "1    1017         WELCH     DARIUS      NEW YORK    NY  212/586-5535   M   \n",
      "7    1076        VENTER    RANDALL      NEW YORK    NY  718/383-2321   M   \n",
      "15   1106     MARSHBURN     JASPER      STAMFORD    CT  203/781-1457   M   \n",
      "16   1107      THOMPSON      WAYNE      NEW YORK    NY  718/384-3785   M   \n",
      "17   1111        RHODES     JEREMY     PRINCETON    NJ  201/812-1837   M   \n",
      "24   1118        DENNIS      ROGER      NEW YORK    NY  718/383-1122   M   \n",
      "32   1126        KIMANI       ANNE      NEW YORK    NY  212/586-1229   F   \n",
      "45   1269        CASTON   FRANKLIN      STAMFORD    CT  203/781-3335   M   \n",
      "47   1332    STEPHENSON       ADAM    BRIDGEPORT    CT  203/675-1497   M   \n",
      "48   1333      BANADYGA     JUSTIN      STAMFORD    CT  203/781-1777   M   \n",
      "49   1347        O'NEAL      BRYAN      NEW YORK    NY  718/384-0230   M   \n",
      "51   1352        RIVERS      SIMON      NEW YORK    NY  718/383-3345   M   \n",
      "56   1379        MORGAN     ALFRED      STAMFORD    CT  203/781-2216   M   \n",
      "58   1385        RAYNOR     MILTON    BRIDGEPORT    CT  203/675-2846   M   \n",
      "65   1404         COHEN        LEE      NEW YORK    NY  718/384-2946   M   \n",
      "68   1407     GREGORSKI     DANIEL    MT. VERNON    NY  914/468-1616   M   \n",
      "70   1409       HAVELKA    RAYMOND      STAMFORD    CT  203/781-9697   M   \n",
      "71   1410        HARRIS    CHARLES      STAMFORD    CT  203/781-0937   M   \n",
      "77   1417       NEWKIRK    WILLIAM      PATERSON    NJ  201/732-6611   M   \n",
      "79   1420         ROUSE     JEREMY      PATERSON    NJ  201/732-9834   M   \n",
      "87   1428         BRADY  CHRISTINE      STAMFORD    CT  203/781-1212   F   \n",
      "98   1439    HASENHAUER  CHRISTINA    BRIDGEPORT    CT  203/675-4987   F   \n",
      "101  1442       NEWKIRK     SANDRA     PRINCETON    NJ  201/812-3331   F   \n",
      "102  1443         WELLS      AGNES      STAMFORD    CT  203/781-5546   F   \n",
      "106  1478        NEWTON      JAMES      NEW YORK    NY  212/587-5549   M   \n",
      "109  1499      BAREFOOT     JOSEPH     PRINCETON    NJ  201/812-5665   M   \n",
      "110  1521        PARKER        JAY      NEW YORK    NY  212/587-7603   M   \n",
      "111  1545       HERRERO      CLYDE      STAMFORD    CT  203/781-1119   M   \n",
      "113  1556    PENNINGTON    MICHAEL      NEW YORK    NY  718/383-5681   M   \n",
      "118  1639  CARTER-COHEN      KAREN      STAMFORD    CT  203/781-8839   F   \n",
      "125  1739    BRANCACCIO     JOSEPH      NEW YORK    NY  212/587-1247   M   \n",
      "126  1777        LUFKIN        ROY      NEW YORK    NY  718/383-4413   M   \n",
      "129  1830         TRIPP      KATHY    BRIDGEPORT    CT  203/675-2479   F   \n",
      "131  1839        NORRIS      DIANE      NEW YORK    NY  718/384-1767   F   \n",
      "134  1882        TUCKER       ALAN      NEW YORK    NY  718/384-0216   M   \n",
      "135  1890    STEPHENSON     ROBERT      NEW YORK    NY  718/384-9874   M   \n",
      "137  1905        GRAHAM      ALVIN      NEW YORK    NY  212/586-8815   M   \n",
      "141  1928      UPCHURCH      LARRY  WHITE PLAINS    NY  914/455-5009   M   \n",
      "142  1935     FERNANDEZ    KATRINA    BRIDGEPORT    CT  203/675-2962   F   \n",
      "\n",
      "    JOBCODE    SALARY      BIRTH      HIRED  \n",
      "1       TA3   40858.0 1957-12-28 1981-10-16  \n",
      "7       PT1   66558.0 1955-10-14 1991-10-03  \n",
      "15      PT2   89632.0 1957-11-06 1984-08-16  \n",
      "16      PT2   89977.0 1954-06-09 1979-02-10  \n",
      "17      NA1   40586.0 1973-07-14 1992-10-31  \n",
      "24      PT3  111379.0 1944-01-16 1980-12-18  \n",
      "32      TA3   40899.0 1963-05-28 1980-11-21  \n",
      "45      NA1   41690.0 1972-05-03 1992-11-28  \n",
      "47      NA1   42178.0 1970-09-17 1991-06-04  \n",
      "48      PT2   88606.0 1961-03-30 1981-02-10  \n",
      "49      TA3   40079.0 1967-09-21 1984-09-06  \n",
      "51      NA2   53798.0 1960-12-02 1986-10-16  \n",
      "56      ME3   42264.0 1961-08-08 1984-06-10  \n",
      "58      ME3   43900.0 1962-01-16 1986-04-01  \n",
      "65      PT2   91376.0 1953-02-24 1980-01-01  \n",
      "68      PT1   68096.0 1969-03-23 1990-03-18  \n",
      "70      ME3   41551.0 1950-04-19 1981-10-22  \n",
      "71      PT2   84685.0 1967-05-03 1986-11-07  \n",
      "77      NA2   52270.0 1964-06-27 1989-03-07  \n",
      "79      ME3   43071.0 1965-02-19 1987-07-22  \n",
      "87      PT1   68767.0 1960-04-04 1991-11-16  \n",
      "98      PT1   70736.0 1964-03-06 1990-09-10  \n",
      "101     PT2   84536.0 1966-09-05 1988-04-12  \n",
      "102     NA1   42274.0 1968-11-17 1991-08-29  \n",
      "106     PT2   84203.0 1959-08-09 1990-10-24  \n",
      "109     ME3   43025.0 1954-04-26 1980-06-07  \n",
      "110     ME3   41526.0 1963-04-12 1988-07-13  \n",
      "111     PT1   66130.0 1959-08-12 1990-05-29  \n",
      "113     PT1   71349.0 1964-06-22 1991-12-11  \n",
      "118     TA3   40260.0 1957-06-26 1984-01-28  \n",
      "125     PT1   66517.0 1964-12-25 1991-01-27  \n",
      "126     PT3  109630.0 1951-09-23 1981-06-21  \n",
      "129     PT2   84471.0 1957-05-27 1983-01-29  \n",
      "131     NA1   43433.0 1970-11-29 1993-07-03  \n",
      "134     ME3   41538.0 1957-07-10 1978-11-21  \n",
      "135     PT2   91908.0 1951-07-20 1979-11-25  \n",
      "137     PT1   65111.0 1972-04-16 1992-05-29  \n",
      "141     PT2   89858.0 1954-09-16 1990-07-13  \n",
      "142     NA2   51081.0 1954-03-28 1981-10-16  \n",
      "Number of rows: 39\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "# Sample 3: Supervisor Information\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Merge DataFrames, renaming 'SUPID' from df8 to 'IDNUM'\n",
    "df_combined = pd.merge(df7, df8.rename(columns={'SUPID': 'IDNUM'}), on='IDNUM', how='inner')\n",
    "\n",
    "# Filter based on likely scenarios\n",
    "\n",
    "# Scenario 1: If 'super' was intended as a boolean filter\n",
    "if 'SUPER' in df_combined.columns:  # Checking for case-insensitive existence\n",
    "    df_filtered = df_combined[df_combined['SUPER'] == True] \n",
    "else:\n",
    "    print(\"Column 'super' (or a case variant) not found. Filtering skipped.\")\n",
    "\n",
    "# Scenario 2: If 'super' indicates presence in SAMDAT8, create a similar column\n",
    "df_combined['SUPER'] = True  # Assuming all merged rows originated in SAMDAT8\n",
    "\n",
    "# Display Result (assuming you want all columns of the filtered data)\n",
    "print(df_filtered) \n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_filtered.shape[0])\n",
    "print(\"Number of columns:\", df_filtered.shape[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 4       */\n",
    " /*=========================*/\n",
    "\n",
    "data work.payroll;\n",
    "  update samples.SAMDAT5\n",
    "         samples.SAMDAT6;\n",
    "  by IDNUM;\n",
    "run;\n",
    "\n",
    "proc print data=work.payroll;\n",
    "  title 'Libname Sample 4: Updated Payroll Data';\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code aims to:\n",
    "\n",
    "Update Dataset: It updates the 'payroll' dataset in the 'work' library by combining information from two datasets: 'SAMDAT5' and 'SAMDAT6' (both in the 'samples' library).\n",
    "Join Condition: The update is performed by matching records based on the 'IDNUM' column.\n",
    "Print Results: It prints the updated 'payroll' dataset with a custom title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_payroll' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_payroll\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Assuming 'df_payroll' exists (either loaded or created as a copy of df5)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df_payroll \u001b[38;5;241m=\u001b[39m update_payroll(\u001b[43mdf_payroll\u001b[49m\u001b[38;5;241m.\u001b[39mcopy(), df5, df6) \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Display Result\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_payroll)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_payroll' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample 4: Updated Payroll Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Updating logic - You'll need to define this based on how you want to update the 'payroll' dataset\n",
    "\n",
    "def update_payroll(df_payroll, df5, df6):\n",
    "    # ... Your logic to update df_payroll based on values in df5 and df6\n",
    "    # Example: Adding a new column from df6\n",
    "    df_payroll['NEW_COLUMN'] = df6['NEW_COLUMN'] \n",
    "    return df_payroll\n",
    "\n",
    "# Assuming 'df_payroll' exists (either loaded or created as a copy of df5)\n",
    "df_payroll = update_payroll(df_payroll.copy(), df5, df6) \n",
    "\n",
    "# Display Result\n",
    "print(df_payroll)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_payroll.shape[0])\n",
    "print(\"Number of columns:\", df_payroll.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 4: Updated Payroll Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Updating logic \n",
    "def update_payroll(df_base, df5, df6):\n",
    "    # ... Your logic to update df_base based on values in df5 and df6\n",
    "\n",
    "    # Example 1: Adding a column directly from df6\n",
    "    df_base['NEW_COLUMN'] = df6['NEW_COLUMN']  \n",
    "\n",
    "    # Example 2: Update with conditions (assuming both df5 and df_base have 'SALARY')\n",
    "    df_base.loc[df_base['IDNUM'].isin(df5['IDNUM']), 'SALARY'] = df5['SALARY']\n",
    "\n",
    "    return df_base\n",
    "\n",
    "# 2. Decide your base dataset\n",
    "base_dataset = 'df5'  # Option 1: Start with df5 and update\n",
    "# base_dataset = 'df6'  # Option 2: Start with df6 and update\n",
    "\n",
    "# 3. Load or create the base DataFrame\n",
    "if base_dataset == 'df5':\n",
    "    df_payroll = df5.copy()\n",
    "else: \n",
    "    df_payroll = df6.copy()\n",
    "\n",
    "# 4. Update\n",
    "df_payroll = update_payroll(df_payroll, df5, df6) \n",
    "\n",
    "# Display Result, dimensions, etc. (rest of your code remains the same)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 5       */\n",
    " /*=========================*/\n",
    "title 'Libname Sample 5: Total Salary by Jobcode';\n",
    "\n",
    "proc sql;\n",
    "  select JOBCODE label='Jobcode',\n",
    "         sum(SALARY) as total\n",
    "         label='Total for Group'\n",
    "         format=dollar11.2\n",
    "  from samples.SAMDAT5\n",
    "  group by JOBCODE;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code calculates the total salary for each unique job code in the 'SAMDAT5' dataset (located in the 'samples' library). It also adds labels and formats the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobcode Total for Group\n",
      "    BCK     $232,148.00\n",
      "    FA1     $253,433.00\n",
      "    FA2     $447,790.00\n",
      "    FA3     $230,537.00\n",
      "    ME1     $228,002.00\n",
      "    ME2     $498,076.00\n",
      "    ME3     $296,875.00\n",
      "    NA1     $210,161.00\n",
      "    NA2     $157,149.00\n",
      "    PT1     $543,264.00\n",
      "    PT2     $879,252.00\n",
      "    PT3     $221,009.00\n",
      "    SCP     $128,162.00\n",
      "    TA1     $249,492.00\n",
      "    TA2     $671,499.00\n",
      "    TA3     $476,155.00\n",
      "Number of rows: 16\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample 5: Total Salary by Jobcode\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# SQL equivalent in Python using Pandas\n",
    "df_grouped = df5.groupby('JOBCODE')['SALARY'].sum()\n",
    "df_grouped = df_grouped.reset_index()  # Reset index to create separate columns\n",
    "df_grouped.columns = ['Jobcode', 'Total for Group']  # Rename columns for clarity\n",
    "df_grouped['Total for Group'] = df_grouped['Total for Group'].apply('${:,.2f}'.format)  # Format currency\n",
    "\n",
    "# Display result\n",
    "print(df_grouped.to_string(index=False))  # Print without index\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_grouped.shape[0])\n",
    "print(\"Number of columns:\", df_grouped.shape[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 6       */\n",
    " /*=========================*/\n",
    "\n",
    "title 'Libname Sample 6: Flights to London and Frankfurt';\n",
    "\n",
    "proc sql;\n",
    "  select DATES, DEST from samples.SAMDAT2\n",
    "  where (DEST eq \"FRA\") or\n",
    "    (DEST eq \"LON\")\n",
    "  order by DEST;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Filters Data: It selects the 'DATES' and 'DEST' columns from the 'SAMDAT2' dataset (located in the 'samples' library) where the 'DEST' column is either \"FRA\" (Frankfurt) or \"LON\" (London).\n",
    "Sorts Results: The results are ordered alphabetically by the 'DEST' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DATES DEST\n",
      "3  1998-03-01  FRA\n",
      "10 1998-03-02  FRA\n",
      "17 1998-03-03  FRA\n",
      "24 1998-03-04  FRA\n",
      "31 1998-03-05  FRA\n",
      "42 1998-03-07  FRA\n",
      "2  1998-03-01  LON\n",
      "9  1998-03-02  LON\n",
      "16 1998-03-03  LON\n",
      "23 1998-03-04  LON\n",
      "30 1998-03-05  LON\n",
      "36 1998-03-06  LON\n",
      "41 1998-03-07  LON\n",
      "Number of rows: 13\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample 6: Flights to London and Frankfurt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Filter for flights to Frankfurt or London\n",
    "df_filtered = df2[df2['DEST'].isin(['FRA', 'LON'])]\n",
    "\n",
    "# Select and sort\n",
    "df_output = df_filtered[['DATES', 'DEST']].sort_values(by='DEST')\n",
    "\n",
    "# Display result\n",
    "print(df_output)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_output.shape[0])\n",
    "print(\"Number of columns:\", df_output.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 7       */\n",
    " /*=========================*/\n",
    "\n",
    "proc sql;\n",
    "   title  'Libname Sample 7: International Flights by Flight Number';\n",
    "   title2 'with Over 200 Passengers';\n",
    "   select FLIGHT   label=\"Flight Number\",\n",
    "          DATES    label=\"Departure Date\",\n",
    "          DEST     label=\"Destination\",\n",
    "          BOARDED  label=\"Number Boarded\"\n",
    "     from samples.SAMDAT3\n",
    "    where BOARDED > 200\n",
    "    order by FLIGHT;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Filters Data: Selects data from the 'SAMDAT3' dataset (located in the 'samples' library) where the 'BOARDED' column is greater than 200 (representing international flights with a high passenger count).\n",
    "Selects Columns/Labels: Selects specific columns ('FLIGHT', 'DATES', 'DEST', 'BOARDED'), applying custom labels for clarity.\n",
    "Orders Results: Sorts the output by the 'FLIGHT' column.\n",
    "Adds Titles: Adds a main title and a subtitle to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 7: International Flights by Flight Number\n",
      "with Over 200 Passengers\n",
      "   Flight Number Departure Date Destination  Number Boarded\n",
      "12           219     1998-03-04         LON           232.0\n",
      "22           219     1998-03-07         LON           241.0\n",
      "1            622     1998-03-01         FRA           207.0\n",
      "23           622     1998-03-07         FRA           210.0\n",
      "Number of rows: 4\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Sample 7: International Flights by Flight Number with Over 200 Passengers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Filter the data\n",
    "df_filtered = df3[df3['BOARDED'] > 200]\n",
    "\n",
    "# Select columns and rename\n",
    "df_output = df_filtered[['FLIGHT', 'DATES', 'DEST', 'BOARDED']]\n",
    "df_output.columns = ['Flight Number', 'Departure Date', 'Destination', 'Number Boarded']\n",
    "\n",
    "# Sort results\n",
    "df_output = df_output.sort_values(by='Flight Number')\n",
    "\n",
    "# Display result (handling titles separately)\n",
    "print('Libname Sample 7: International Flights by Flight Number') \n",
    "print('with Over 200 Passengers')\n",
    "print(df_output)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_output.shape[0])\n",
    "print(\"Number of columns:\", df_output.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 8        */\n",
    " /*=========================*/\n",
    "\n",
    "title 'Libname Sample 8: Employees with salary greater than $40,000';\n",
    "\n",
    "proc sql;\n",
    "  select a.LNAME, a.FNAME, b.SALARY\n",
    "    format=dollar10.2\n",
    "  from samples.SAMDAT7 a, samples.SAMDAT5 b\n",
    "  where (a.IDNUM eq b.IDNUM) and\n",
    "    (b.SALARY gt 40000);\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This code aims to:\n",
    "\n",
    "Join Datasets: Combines information from 'SAMDAT7' (alias 'a') and 'SAMDAT5' (alias 'b') datasets, located in the 'samples' library, based on a common 'IDNUM' column.\n",
    "Filter Data: Selects records where the 'SALARY' in the 'SAMDAT5' dataset is greater than $40,000.\n",
    "Select Columns: Selects the 'LNAME' and 'FNAME' columns from 'SAMDAT7' and the 'SALARY' column from 'SAMDAT5', applying formatting to the 'SALARY'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 8: Employees with salary greater than $40,000\n",
      "            LNAME      FNAME       SALARY\n",
      "1           WELCH     DARIUS   $40,858.00\n",
      "7          VENTER    RANDALL   $66,558.00\n",
      "15      MARSHBURN     JASPER   $89,632.00\n",
      "16       THOMPSON      WAYNE   $89,977.00\n",
      "17         RHODES     JEREMY   $40,586.00\n",
      "24         DENNIS      ROGER  $111,379.00\n",
      "32         KIMANI       ANNE   $40,899.00\n",
      "45         CASTON   FRANKLIN   $41,690.00\n",
      "47     STEPHENSON       ADAM   $42,178.00\n",
      "48       BANADYGA     JUSTIN   $88,606.00\n",
      "49         O'NEAL      BRYAN   $40,079.00\n",
      "51         RIVERS      SIMON   $53,798.00\n",
      "56         MORGAN     ALFRED   $42,264.00\n",
      "58         RAYNOR     MILTON   $43,900.00\n",
      "65          COHEN        LEE   $91,376.00\n",
      "68      GREGORSKI     DANIEL   $68,096.00\n",
      "70        HAVELKA    RAYMOND   $41,551.00\n",
      "71         HARRIS    CHARLES   $84,685.00\n",
      "77        NEWKIRK    WILLIAM   $52,270.00\n",
      "79          ROUSE     JEREMY   $43,071.00\n",
      "87          BRADY  CHRISTINE   $68,767.00\n",
      "98     HASENHAUER  CHRISTINA   $70,736.00\n",
      "101       NEWKIRK     SANDRA   $84,536.00\n",
      "102         WELLS      AGNES   $42,274.00\n",
      "106        NEWTON      JAMES   $84,203.00\n",
      "109      BAREFOOT     JOSEPH   $43,025.00\n",
      "110        PARKER        JAY   $41,526.00\n",
      "111       HERRERO      CLYDE   $66,130.00\n",
      "113    PENNINGTON    MICHAEL   $71,349.00\n",
      "118  CARTER-COHEN      KAREN   $40,260.00\n",
      "125    BRANCACCIO     JOSEPH   $66,517.00\n",
      "126        LUFKIN        ROY  $109,630.00\n",
      "129         TRIPP      KATHY   $84,471.00\n",
      "131        NORRIS      DIANE   $43,433.00\n",
      "134        TUCKER       ALAN   $41,538.00\n",
      "135    STEPHENSON     ROBERT   $91,908.00\n",
      "137        GRAHAM      ALVIN   $65,111.00\n",
      "141      UPCHURCH      LARRY   $89,858.00\n",
      "142     FERNANDEZ    KATRINA   $51,081.00\n",
      "Number of rows: 39\n",
      "Number of columns: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\comma\\AppData\\Local\\Temp\\ipykernel_8500\\2796675738.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output['SALARY'] = df_output['SALARY'].apply('${:,.2f}'.format)\n"
     ]
    }
   ],
   "source": [
    "# Sample 8: Employees with salary greater than $40,000\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Join DataFrames on 'IDNUM'\n",
    "df_joined = pd.merge(df7, df5, on='IDNUM', how='inner')\n",
    "\n",
    "# Filter for salary \n",
    "df_filtered = df_joined[df_joined['SALARY'] > 40000]\n",
    "\n",
    "# Select columns\n",
    "df_output = df_filtered[['LNAME', 'FNAME', 'SALARY']]\n",
    "\n",
    "# Format SALARY column\n",
    "df_output['SALARY'] = df_output['SALARY'].apply('${:,.2f}'.format)\n",
    "\n",
    "# Display results\n",
    "print('Libname Sample 8: Employees with salary greater than $40,000')\n",
    "print(df_output)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_output.shape[0])\n",
    "print(\"Number of columns:\", df_output.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 9         */\n",
    " /*==========================*/\n",
    "\n",
    "/* SQL Implicit Passthru ON */\n",
    "title 'Libname Sample 9a: Delayed International Flights in March';\n",
    "\n",
    "proc sql;\n",
    "  select distinct samdat1.FLIGHT,\n",
    "      samdat1.DATES,\n",
    "      DELAY format=2.0\n",
    "    from samples.SAMDAT1, samples.SAMDAT2, samples.SAMDAT3\n",
    "  where samdat1.FLIGHT=samdat2.FLIGHT and\n",
    "        samdat1.DATES=samdat2.DATES and\n",
    "        samdat1.FLIGHT=samdat3.FLIGHT and\n",
    "        DELAY>0\n",
    "  order by DELAY descending;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code aims to:\n",
    "\n",
    "Join Multiple Datasets: Combines information from 'SAMDAT1', 'SAMDAT2', and 'SAMDAT3' (all in the 'samples' library) based on matching 'FLIGHT' and 'DATES' columns.\n",
    "Filter for Delays: Selects only records where the 'DELAY' column in 'SAMDAT1' is greater than 0, indicating a delayed flight.\n",
    "Prioritize International Flights: Implicitly assumes that international flights would be present in the 'SAMDAT2' or 'SAMDAT3' datasets.\n",
    "Select Columns: Selects and displays 'FLIGHT', 'DATES', and 'DELAY' with formatting applied to the 'DELAY' column.\n",
    "Order Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 9a: Delayed International Flights in March\n",
      "   FLIGHT      DATES  DELAY\n",
      "23    622 1998-03-04   30.0\n",
      "12    219 1998-03-06   27.0\n",
      "25    622 1998-03-07   21.0\n",
      "8     219 1998-03-02   18.0\n",
      "7     219 1998-03-01   18.0\n",
      "13    219 1998-03-07   15.0\n",
      "0     132 1998-03-01   14.0\n",
      "5     132 1998-03-06    7.0\n",
      "2     132 1998-03-03    6.0\n",
      "1     132 1998-03-02    5.0\n",
      "14    271 1998-03-01    5.0\n",
      "17    271 1998-03-04    5.0\n",
      "18    271 1998-03-05    5.0\n",
      "9     219 1998-03-03    4.0\n",
      "15    271 1998-03-02    4.0\n",
      "19    271 1998-03-07    4.0\n",
      "10    219 1998-03-04    3.0\n",
      "11    219 1998-03-05    3.0\n",
      "4     132 1998-03-05    3.0\n",
      "16    271 1998-03-03    2.0\n",
      "Number of rows: 20\n",
      "Number of columns: 3\n"
     ]
    }
   ],
   "source": [
    "# Sample 9a: Delayed International Flights in March (Approximate Conversion)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Join datasets (assuming 'FLIGHT' and 'DATES' columns allow for proper matching)\n",
    "df_joined = pd.merge(df1, df2, on=['FLIGHT', 'DATES'], how='inner')\n",
    "df_joined = pd.merge(df_joined, df3, on=['FLIGHT', 'DATES'], how='inner')\n",
    "\n",
    "# Filter for delays\n",
    "df_filtered = df_joined[df_joined['DELAY'] > 0]\n",
    "\n",
    "# Select columns and sort\n",
    "df_output = df_filtered[['FLIGHT', 'DATES', 'DELAY']].sort_values(by='DELAY', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print('Libname Sample 9a: Delayed International Flights in March') \n",
    "print(df_output)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_output.shape[0])\n",
    "print(\"Number of columns:\", df_output.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 9c        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 9c: Delayed International Flights in March';\n",
    "\n",
    "proc sql;\n",
    "  select distinct samdat1.FLIGHT,\n",
    "      samdat1.DATES,\n",
    "      DELAY format=2.0\n",
    "    from samples.SAMDAT1\n",
    "    full join samples.SAMDAT2 on\n",
    "      samdat1.FLIGHT = samdat2.FLIGHT\n",
    "    full join samples.SAMDAT3 on\n",
    "      samdat1.FLIGHT = samdat3.FLIGHT\n",
    "  order by DELAY descending;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code modifies the previous example with these key changes:\n",
    "\n",
    "Full Joins: Utilizes FULL JOIN operations to include flights potentially present in only one of the datasets. This might bring in more flight records than the previous inner joins.\n",
    "Implicit Filtering: While not explicit, the focus is likely still on delayed international flights. We'll still need to determine how to identify international flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 9c: Delayed International Flights in March\n",
      "     FLIGHT      DATES  DELAY\n",
      "1090    622 1998-03-01   30.0\n",
      "1130    622 1998-03-05   30.0\n",
      "1202    622 1998-03-05   30.0\n",
      "1201    622 1998-03-04   30.0\n",
      "1200    622 1998-03-03   30.0\n",
      "...     ...        ...    ...\n",
      "1010    302        NaT    1.0\n",
      "1016    302        NaT    1.0\n",
      "1022    302        NaT    1.0\n",
      "1028    302        NaT    1.0\n",
      "1034    302        NaT    1.0\n",
      "\n",
      "[922 rows x 3 columns]\n",
      "Number of rows: 922\n",
      "Number of columns: 3\n"
     ]
    }
   ],
   "source": [
    "# Sample 9c: Delayed International Flights in March (Approximate Conversion)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Join datasets (using 'outer' to approximate FULL JOIN)\n",
    "df_joined = pd.merge(df1, df2, on='FLIGHT', how='outer')\n",
    "df_joined = pd.merge(df_joined, df3, on='FLIGHT', how='outer')\n",
    "\n",
    "# Filter for delays (assuming DELAY comes from df1)\n",
    "df_filtered = df_joined[df_joined['DELAY'] > 0]\n",
    "\n",
    "# Select columns and sort\n",
    "df_output = df_filtered[['FLIGHT', 'DATES', 'DELAY']].sort_values(by='DELAY', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print('Libname Sample 9c: Delayed International Flights in March') \n",
    "print(df_output)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_output.shape[0])\n",
    "print(\"Number of columns:\", df_output.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*==========================*/\n",
    " /* LIBNAME Sample 10        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 10: Payrolls 1 & 2';\n",
    "\n",
    "proc sql;\n",
    "  select IDNUM, SEX, JOBCODE, SALARY,\n",
    "         BIRTH,\n",
    "         HIRED\n",
    "     from samples.SAMDAT5\n",
    "  outer union corr\n",
    "  select *\n",
    "     from samples.SAMDAT6\n",
    "   order by IDNUM, JOBCODE, SALARY;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code aims to:\n",
    "\n",
    "Combine Datasets: Merge information from 'SAMDAT5' and 'SAMDAT6' (both in the 'samples' library) while potentially preserving rows from both datasets even if there are no matches.\n",
    "Select Columns: Select specific columns: 'IDNUM', 'SEX', 'JOBCODE', 'SALARY', 'BIRTH', and 'HIRED'.\n",
    "Ordering: Sort the result by 'IDNUM', 'JOBCODE', and 'SALARY' in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 10: Payrolls 1 & 2\n",
      "    IDNUM SEX JOBCODE   SALARY      BIRTH      HIRED\n",
      "0    1009   M     TA1  28880.0 1959-03-02 1992-03-26\n",
      "1    1017   M     TA3  40858.0 1957-12-28 1981-10-16\n",
      "2    1036   F     TA3  39392.0 1965-05-19 1984-10-23\n",
      "148  1036   F     TA3  42465.0 1965-05-19 1984-10-23\n",
      "3    1037   F     TA1  28558.0 1964-04-10 1992-09-13\n",
      "..    ...  ..     ...      ...        ...        ...\n",
      "144  1983   F     FA3  33419.0 1962-02-28 1987-04-27\n",
      "145  1988   M     FA3  32217.0 1959-11-30 1984-09-18\n",
      "146  1991   F     TA1  27645.0 1972-05-07 1992-12-12\n",
      "147  1995   F     ME1  28810.0 1973-08-24 1993-09-19\n",
      "159  1998   M     SCP  23100.0 1970-09-10 1992-11-02\n",
      "\n",
      "[160 rows x 6 columns]\n",
      "Number of rows: 160\n",
      "Number of columns: 6\n"
     ]
    }
   ],
   "source": [
    "# Sample 10: Payrolls 1 & 2 \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate DataFrames, potentially including non-matching rows \n",
    "df_combined = pd.concat([df5, df6], axis=0, ignore_index=True) \n",
    "\n",
    "# Select columns\n",
    "df_result = df_combined[['IDNUM', 'SEX', 'JOBCODE', 'SALARY', 'BIRTH', 'HIRED']] \n",
    "\n",
    "# Sort the results \n",
    "df_result = df_result.sort_values(by=['IDNUM', 'JOBCODE', 'SALARY'])\n",
    "\n",
    "# Display results\n",
    "print('Libname Sample 10: Payrolls 1 & 2') \n",
    "print(df_result)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_result.shape[0])\n",
    "print(\"Number of columns:\", df_result.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 11        */\n",
    " /*==========================*/\n",
    "\n",
    "proc sql undo_policy=none;\n",
    "insert into samples.SAMDAT8\n",
    "\tvalues('1588','NY','FA');\n",
    "quit;\n",
    "\n",
    "proc print data=samples.SAMDAT8;\n",
    "\ttitle 'Libname Sample 11: New Row in AIRLINE.SAMDAT8';\n",
    "run;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "Disables Undo Policy: The statement proc sql undo_policy=none;  temporarily disables the ability to undo this SQL operation. This is often done for performance reasons.\n",
    "Inserts Data: It inserts a new row with the values '1588', 'NY', and 'FA' into the 'SAMDAT8' dataset (located in the 'samples' library).\n",
    "Prints Results: Prints the 'SAMDAT8' dataset to display the newly inserted row with a custom title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8500\\2846185869.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# **Caution:** Consider the implications of disabling undo behavior in your context.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Insert (assumes your DataFrame is tied to a database where this will persist)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdf8\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Display results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Libname Sample 11: New Row in AIRLINE.SAMDAT8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6293\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6294\u001b[0m         ):\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6296\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Sample 11: New Row in AIRLINE.SAMDAT8\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# New row data (assuming df8 has appropriate columns to match these)\n",
    "new_row = {'IDNUM': '1588', 'STATE': 'NY', 'JOBCODE': 'FA'} # Adjust column names if needed\n",
    "\n",
    "# **Caution:** Consider the implications of disabling undo behavior in your context. \n",
    "\n",
    "# Insert (assumes your DataFrame is tied to a database where this will persist)\n",
    "df8 = df8.append(new_row, ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print('Libname Sample 11: New Row in AIRLINE.SAMDAT8')\n",
    "print(df8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 11: New Row in AIRLINE.SAMDAT8\n",
      "   SUPID STATE JOBCAT IDNUM JOBCODE\n",
      "0   1106    CT     PT   NaN     NaN\n",
      "1   1118    NY     PT   NaN     NaN\n",
      "2   1126    NY     TA   NaN     NaN\n",
      "3   1352    NY     NA   NaN     NaN\n",
      "4   1385    CT     ME   NaN     NaN\n",
      "5   1401    NJ     TA   NaN     NaN\n",
      "6   1405    NJ     SC   NaN     NaN\n",
      "7   1417    NJ     NA   NaN     NaN\n",
      "8   1420    NJ     ME   NaN     NaN\n",
      "9   1431    CT     FA   NaN     NaN\n",
      "10  1433    NJ     FA   NaN     NaN\n",
      "11  1442    NJ     PT   NaN     NaN\n",
      "12  1564    NY     SC   NaN     NaN\n",
      "13  1639    CT     TA   NaN     NaN\n",
      "14  1677    CT     BC   NaN     NaN\n",
      "15  1834    NY     BC   NaN     NaN\n",
      "16  1882    NY     ME   NaN     NaN\n",
      "17  1935    CT     NA   NaN     NaN\n",
      "18  1983    NY     FA   NaN     NaN\n",
      "19   NaN    NY    NaN  1588      FA\n"
     ]
    }
   ],
   "source": [
    "# Sample 11: New Row in AIRLINE.SAMDAT8\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# New row data (assuming df8 has appropriate columns to match these)\n",
    "new_row = {'IDNUM': '1588', 'STATE': 'NY', 'JOBCODE': 'FA'} # Adjust column names if needed\n",
    "\n",
    "# Create a temporary DataFrame from the new row\n",
    "temp_df = pd.DataFrame([new_row])\n",
    "\n",
    "# Concatenate with the original DataFrame\n",
    "df8 = pd.concat([df8, temp_df], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print('Libname Sample 11: New Row in AIRLINE.SAMDAT8')\n",
    "print(df8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 13        */\n",
    " /*==========================*/\n",
    "\n",
    "proc sql;\n",
    "\n",
    "  create table work.gtforty as\n",
    "  select LNAME as lastname,\n",
    "         FNAME as firstname,\n",
    "         SALARY as Salary\n",
    "  from samples.SAMDAT7 a, samples.SAMDAT5 b\n",
    "  where (a.IDNUM eq b.IDNUM) and (SALARY gt 40000);\n",
    "\n",
    "quit;\n",
    "\n",
    "proc print data=work.gtforty noobs;\n",
    "  title 'Libname Sample 13: Employees with salaries over $40,000';\n",
    "  format SALARY dollar10.2;\n",
    "\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Creates a New Dataset: It creates a dataset named 'gtforty' in the 'work' library. This dataset is derived from the 'SAMDAT7' and 'SAMDAT5' datasets (both in the 'samples' library).\n",
    "Filtering and Column Selection: It includes only rows where:\n",
    "'IDNUM' values match between 'SAMDAT7' and 'SAMDAT5'.\n",
    "The 'SALARY' (from 'SAMDAT5') is greater than $40,000.\n",
    "It selects the 'LNAME', 'FNAME', and 'SALARY' columns, renaming them for clarity.\n",
    "Prints Results: It prints the newly created 'gtforty' dataset with a custom title and formats the 'SALARY' column as currency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 13: Employees with salaries over $40,000\n",
      "         lastname  firstname       Salary\n",
      "1           WELCH     DARIUS   $40,858.00\n",
      "7          VENTER    RANDALL   $66,558.00\n",
      "15      MARSHBURN     JASPER   $89,632.00\n",
      "16       THOMPSON      WAYNE   $89,977.00\n",
      "17         RHODES     JEREMY   $40,586.00\n",
      "24         DENNIS      ROGER  $111,379.00\n",
      "32         KIMANI       ANNE   $40,899.00\n",
      "45         CASTON   FRANKLIN   $41,690.00\n",
      "47     STEPHENSON       ADAM   $42,178.00\n",
      "48       BANADYGA     JUSTIN   $88,606.00\n",
      "49         O'NEAL      BRYAN   $40,079.00\n",
      "51         RIVERS      SIMON   $53,798.00\n",
      "56         MORGAN     ALFRED   $42,264.00\n",
      "58         RAYNOR     MILTON   $43,900.00\n",
      "65          COHEN        LEE   $91,376.00\n",
      "68      GREGORSKI     DANIEL   $68,096.00\n",
      "70        HAVELKA    RAYMOND   $41,551.00\n",
      "71         HARRIS    CHARLES   $84,685.00\n",
      "77        NEWKIRK    WILLIAM   $52,270.00\n",
      "79          ROUSE     JEREMY   $43,071.00\n",
      "87          BRADY  CHRISTINE   $68,767.00\n",
      "98     HASENHAUER  CHRISTINA   $70,736.00\n",
      "101       NEWKIRK     SANDRA   $84,536.00\n",
      "102         WELLS      AGNES   $42,274.00\n",
      "106        NEWTON      JAMES   $84,203.00\n",
      "109      BAREFOOT     JOSEPH   $43,025.00\n",
      "110        PARKER        JAY   $41,526.00\n",
      "111       HERRERO      CLYDE   $66,130.00\n",
      "113    PENNINGTON    MICHAEL   $71,349.00\n",
      "118  CARTER-COHEN      KAREN   $40,260.00\n",
      "125    BRANCACCIO     JOSEPH   $66,517.00\n",
      "126        LUFKIN        ROY  $109,630.00\n",
      "129         TRIPP      KATHY   $84,471.00\n",
      "131        NORRIS      DIANE   $43,433.00\n",
      "134        TUCKER       ALAN   $41,538.00\n",
      "135    STEPHENSON     ROBERT   $91,908.00\n",
      "137        GRAHAM      ALVIN   $65,111.00\n",
      "141      UPCHURCH      LARRY   $89,858.00\n",
      "142     FERNANDEZ    KATRINA   $51,081.00\n",
      "Number of rows: 39\n",
      "Number of columns: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\comma\\AppData\\Local\\Temp\\ipykernel_8500\\214141244.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_gtforty['Salary'] = df_gtforty['Salary'].apply('${:,.2f}'.format)\n"
     ]
    }
   ],
   "source": [
    "# Sample 13: Employees with salaries over $40,000\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Join and filter data\n",
    "df_joined = pd.merge(df7, df5, on='IDNUM', how='inner')\n",
    "df_filtered = df_joined[df_joined['SALARY'] > 40000]\n",
    "\n",
    "# Select and rename columns\n",
    "df_gtforty = df_filtered[['LNAME', 'FNAME', 'SALARY']]\n",
    "df_gtforty.columns = ['lastname', 'firstname', 'Salary']\n",
    "\n",
    "# Format salary\n",
    "df_gtforty['Salary'] = df_gtforty['Salary'].apply('${:,.2f}'.format)\n",
    "\n",
    "# Display results\n",
    "print('Libname Sample 13: Employees with salaries over $40,000')\n",
    "print(df_gtforty)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_gtforty.shape[0])\n",
    "print(\"Number of columns:\", df_gtforty.shape[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 14        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 14: Number of Passengers per Flight by Date';\n",
    "\n",
    "proc print data=samples.SAMDAT1 noobs;\n",
    "  var DATES BOARDED;\n",
    "  by FLIGHT DEST;\n",
    "  sumby FLIGHT;\n",
    "  sum BOARDED;\n",
    "run;\n",
    "\n",
    "title 'Libname Sample 14: Maximum Number of Passengers per Flight';\n",
    "\n",
    "proc means data=samples.SAMDAT1 fw=5 maxdec=1 max;\n",
    "  var BOARDED;\n",
    "  class FLIGHT;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1\n",
    "\n",
    "Calculates Passenger Totals: It calculates the total number of passengers for each combination of 'FLIGHT' and 'DESTINATION' from the 'SAMDAT1' dataset. It also calculates subtotals of passengers for each unique 'FLIGHT'.\n",
    "Prints Results: Prints the results with a custom title, suppressing the 'obs' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Series.rename() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m df_subtotals \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFLIGHT\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBOARDED\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mreset_index()  \u001b[38;5;66;03m# Create subtotals by 'FLIGHT'\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Combine grouped results and subtotals, renaming a column for clarity\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df_part1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_grouped\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBOARDED\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL_PER_FLIGHT_DEST\u001b[39m\u001b[38;5;124m'\u001b[39m}), \u001b[43msubtotals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBOARDED\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTOTAL_PER_FLIGHT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m]) \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLibname Sample 14: Number of Passengers per Flight by Date\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_part1)\n",
      "\u001b[1;31mTypeError\u001b[0m: Series.rename() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "# Sample 14: Number of Passengers per Flight by Date, and Maximum Passengers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Part 1: Number of Passengers per Flight by Date\n",
    "df_grouped = df1.groupby(['FLIGHT', 'DEST'])['BOARDED'].sum()\n",
    "df_grouped = df_grouped.reset_index()  # Convert group labels into columns\n",
    "\n",
    "df_subtotals = df1.groupby(['FLIGHT'])['BOARDED'].sum().reset_index()  # Create subtotals by 'FLIGHT'\n",
    "\n",
    "# Combine grouped results and subtotals, renaming a column for clarity\n",
    "df_part1 = pd.concat([df_grouped.rename(columns={'BOARDED': 'TOTAL_PER_FLIGHT_DEST'}), subtotals.rename(columns={'BOARDED': 'TOTAL_PER_FLIGHT'})]) \n",
    "\n",
    "print('Libname Sample 14: Number of Passengers per Flight by Date')\n",
    "print(df_part1)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_part1.shape[0])\n",
    "print(\"Number of columns:\", df_part1.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 14: Number of Passengers per Flight by Date\n",
      "  FLIGHT DEST  TOTAL_PER_FLIGHT_DEST FLIGHT  TOTAL_PER_FLIGHT\n",
      "0    114  LAX                 1071.0    114            1071.0\n",
      "1    132  YYZ                  884.0    132             884.0\n",
      "2    202  ORD                  931.0    202             931.0\n",
      "3    219  LON                 1338.0    219            1338.0\n",
      "4    271  PAR                  867.0    271             867.0\n",
      "5    302  WAS                  622.0    302             622.0\n",
      "6    622  FRA                 1095.0    622            1095.0\n",
      "Number of rows: 7\n",
      "Number of columns: 5\n"
     ]
    }
   ],
   "source": [
    "# Sample 14: Number of Passengers per Flight by Date\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Part 1: Number of Passengers per Flight by Date (corrected renaming)\n",
    "df_grouped = df1.groupby(['FLIGHT', 'DEST'])['BOARDED'].sum().reset_index() \n",
    "df_grouped = df_grouped.rename(columns={'BOARDED': 'TOTAL_PER_FLIGHT_DEST'})\n",
    "\n",
    "df_subtotals = df1.groupby(['FLIGHT'])['BOARDED'].sum().reset_index()\n",
    "df_subtotals = df_subtotals.rename(columns={'BOARDED': 'TOTAL_PER_FLIGHT'})\n",
    "\n",
    "df_part1 = pd.concat([df_grouped, df_subtotals], axis=1)  # Concatenate along columns\n",
    "\n",
    "print('Libname Sample 14: Number of Passengers per Flight by Date')\n",
    "print(df_part1)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_part1.shape[0])\n",
    "print(\"Number of columns:\", df_part1.shape[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "Calculates Maximum Passengers: It calculates the maximum number of passengers per 'FLIGHT' from the 'SAMDAT1' dataset.\n",
    "Formats Output: Specifies display with up to 5 digits before the decimal and a maximum of 1 decimal place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 14: Maximum Number of Passengers per Flight\n",
      "  FLIGHT  BOARDED\n",
      "0    114      197\n",
      "1    132      164\n",
      "2    202      175\n",
      "3    219      241\n",
      "4    271      177\n",
      "5    302      135\n",
      "6    622      210\n",
      "Number of rows: 7\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Maximum Number of Passengers per Flight\n",
    "df_max = df1.groupby(['FLIGHT'])['BOARDED'].max().reset_index()\n",
    "df_max['BOARDED'] = df_max['BOARDED'].astype(int)  # Adjust formatting if needed\n",
    "\n",
    "print('Libname Sample 14: Maximum Number of Passengers per Flight')\n",
    "print(df_max)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_max.shape[0])\n",
    "print(\"Number of columns:\", df_max.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 16       */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 16: Contents of the SAMDAT2 Table';\n",
    "\n",
    "proc contents data=samples.SAMDAT2;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays Dataset Metadata: It uses the proc contents procedure to generate a report describing the structure of the 'SAMDAT2' dataset (located in the 'samples' library). This includes information like:\n",
    "Variable/column names\n",
    "Data types\n",
    "Column lengths\n",
    "Dataset attributes (e.g., number of observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   ORIG      46 non-null     object        \n",
      " 3   DEST      46 non-null     object        \n",
      " 4   DELAYCAT  46 non-null     object        \n",
      " 5   DESTYPE   46 non-null     object        \n",
      " 6   DELAY     46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(5)\n",
      "memory usage: 2.6+ KB\n",
      "None\n",
      "                               DATES      DELAY\n",
      "count                             46  46.000000\n",
      "mean   1998-03-03 21:23:28.695652224   4.326087\n",
      "min              1998-03-01 00:00:00  -6.000000\n",
      "25%              1998-03-02 00:00:00  -1.000000\n",
      "50%              1998-03-04 00:00:00   3.000000\n",
      "75%              1998-03-05 18:00:00   5.750000\n",
      "max              1998-03-07 00:00:00  30.000000\n",
      "std                              NaN   8.305432\n",
      "Number of rows: 46\n",
      "Number of columns: 7\n"
     ]
    }
   ],
   "source": [
    "# Sample 16: Contents of the SAMDAT2 Table\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Key Metadata\n",
    "print(df2.info())  # Information about columns, data types, non-null counts\n",
    "print(df2.describe())  # Summary statistics for numerical columns \n",
    "\n",
    "# Additional Attributes\n",
    "print(\"Number of rows:\", df2.shape[0])\n",
    "print(\"Number of columns:\", df2.shape[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*==========================*/\n",
    " /* LIBNAME Sample 17        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 17: Ranking of Delayed Flights';\n",
    "\n",
    "options pageno=1;\n",
    "\n",
    "proc rank data=samples.SAMDAT2 descending\n",
    "    ties=low out=work.ranked;\n",
    "  var DELAY;\n",
    "  ranks RANKING;\n",
    "run;\n",
    "\n",
    "proc print data=work.ranked;\n",
    "  format DELAY 2.0;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Ranking Delayed Flights:\n",
    "Ranks flights in the 'SAMDAT2' dataset (from the 'samples' library) based on their 'DELAY' value in descending order (highest delays first).\n",
    "Handles ties by assigning the lowest rank to all tied observations (ties=low).\n",
    "Creates a new dataset named 'ranked' in the 'work' library to store the ranking results.\n",
    "Prints Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 17: Ranking of Delayed Flights\n",
      "   FLIGHT      DATES ORIG DEST      DELAYCAT        DESTYPE  DELAY  RANKING\n",
      "0     114 1998-03-01  LGA  LAX  1-10 Minutes       Domestic   8.00      9.0\n",
      "1     202 1998-03-01  LGA  ORD      No Delay       Domestic  -5.00     42.0\n",
      "2     219 1998-03-01  LGA  LON   11+ Minutes  International  18.00      4.0\n",
      "3     622 1998-03-01  LGA  FRA      No Delay  International  -5.00     42.0\n",
      "4     132 1998-03-01  LGA  YYZ   11+ Minutes  International  14.00      8.0\n",
      "5     271 1998-03-01  LGA  PAR  1-10 Minutes  International   5.00     13.0\n",
      "6     302 1998-03-01  LGA  WAS      No Delay       Domestic  -2.00     36.0\n",
      "7     114 1998-03-02  LGA  LAX      No Delay       Domestic   0.00     28.0\n",
      "8     202 1998-03-02  LGA  ORD  1-10 Minutes       Domestic   5.00     13.0\n",
      "9     219 1998-03-02  LGA  LON   11+ Minutes  International  18.00      4.0\n",
      "10    622 1998-03-02  LGA  FRA      No Delay  International   0.00     28.0\n",
      "11    132 1998-03-02  LGA  YYZ  1-10 Minutes  International   5.00     13.0\n",
      "12    271 1998-03-02  LGA  PAR  1-10 Minutes  International   4.00     19.0\n",
      "13    302 1998-03-02  LGA  WAS      No Delay       Domestic   0.00     28.0\n",
      "14    114 1998-03-03  LGA  LAX      No Delay       Domestic  -1.00     32.0\n",
      "15    202 1998-03-03  LGA  ORD      No Delay       Domestic  -1.00     32.0\n",
      "16    219 1998-03-03  LGA  LON  1-10 Minutes  International   4.00     19.0\n",
      "17    622 1998-03-03  LGA  FRA      No Delay  International  -2.00     36.0\n",
      "18    132 1998-03-03  LGA  YYZ  1-10 Minutes  International   6.00     12.0\n",
      "19    271 1998-03-03  LGA  PAR  1-10 Minutes  International   2.00     25.0\n",
      "20    302 1998-03-03  LGA  WAS  1-10 Minutes       Domestic   5.00     13.0\n",
      "21    114 1998-03-04  LGA  LAX   11+ Minutes       Domestic  15.00      6.0\n",
      "22    202 1998-03-04  LGA  ORD      No Delay       Domestic  -5.00     42.0\n",
      "23    219 1998-03-04  LGA  LON  1-10 Minutes  International   3.00     22.0\n",
      "24    622 1998-03-04  LGA  FRA   11+ Minutes  International  30.00      1.0\n",
      "25    132 1998-03-04  LGA  YYZ      No Delay  International  -5.00     42.0\n",
      "26    271 1998-03-04  LGA  PAR  1-10 Minutes  International   5.00     13.0\n",
      "27    302 1998-03-04  LGA  WAS  1-10 Minutes       Domestic   7.00     10.0\n",
      "28    114 1998-03-05  LGA  LAX      No Delay       Domestic  -2.00     36.0\n",
      "29    202 1998-03-05  LGA  ORD  1-10 Minutes       Domestic   2.00     25.0\n",
      "30    219 1998-03-05  LGA  LON  1-10 Minutes  International   3.00     22.0\n",
      "31    622 1998-03-05  LGA  FRA      No Delay  International  -6.00     46.0\n",
      "32    132 1998-03-05  LGA  YYZ  1-10 Minutes  International   3.00     22.0\n",
      "33    271 1998-03-05  LGA  PAR  1-10 Minutes  International   5.00     13.0\n",
      "34    114 1998-03-06  LGA  LAX      No Delay       Domestic  -1.00     32.0\n",
      "35    202 1998-03-06  LGA  ORD      No Delay       Domestic  -3.00     41.0\n",
      "36    219 1998-03-06  LGA  LON   11+ Minutes  International  27.00      2.0\n",
      "37    132 1998-03-06  LGA  YYZ  1-10 Minutes  International   7.00     10.0\n",
      "38    302 1998-03-06  LGA  WAS  1-10 Minutes       Domestic   1.00     27.0\n",
      "39    114 1998-03-07  LGA  LAX      No Delay       Domestic  -1.00     32.0\n",
      "40    202 1998-03-07  LGA  ORD      No Delay       Domestic  -2.00     36.0\n",
      "41    219 1998-03-07  LGA  LON   11+ Minutes  International  15.00      6.0\n",
      "42    622 1998-03-07  LGA  FRA   11+ Minutes  International  21.00      3.0\n",
      "43    132 1998-03-07  LGA  YYZ      No Delay  International  -2.00     36.0\n",
      "44    271 1998-03-07  LGA  PAR  1-10 Minutes  International   4.00     19.0\n",
      "45    302 1998-03-07  LGA  WAS      No Delay       Domestic   0.00     28.0\n",
      "Number of rows: 46\n",
      "Number of columns: 8\n"
     ]
    }
   ],
   "source": [
    "# Sample 17: Ranking of Delayed Flights\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a copy to preserve the original data \n",
    "df_ranked = df2.copy()\n",
    "\n",
    "# Ranking (Descending order, ties assigned the lowest rank)\n",
    "df_ranked['RANKING'] = df_ranked['DELAY'].rank(ascending=False, method='min')\n",
    "\n",
    "# Print results with formatting (if needed)\n",
    "df_ranked['DELAY'] = df_ranked['DELAY'].apply('{:.2f}'.format)  \n",
    "\n",
    "print('Libname Sample 17: Ranking of Delayed Flights')\n",
    "print(df_ranked)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_ranked.shape[0])\n",
    "print(\"Number of columns:\", df_ranked.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*==========================*/\n",
    " /* LIBNAME Sample 18        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 18: Number of Employees by Jobcode';\n",
    "\n",
    "proc tabulate data=samples.SAMDAT5 format=3.0;\n",
    "   class JOBCODE;\n",
    "   table JOBCODE*n;\n",
    "   keylabel n=\"#\";\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Generates Frequency Table: It creates a frequency table from the 'SAMDAT5' dataset (located in the 'samples' library) that counts the number of occurrences of each unique 'JOBCODE'.\n",
    "Formatting:\n",
    "It specifies a format of '3.0' for the output, likely controlling the display of the counts.\n",
    "It customizes the label for the count column to be \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 18: Number of Employees by Jobcode\n",
      "   JOBCODE   #\n",
      "0      TA2  20\n",
      "1      FA2  16\n",
      "2      ME2  14\n",
      "3      TA3  12\n",
      "4      FA1  11\n",
      "5      PT2  10\n",
      "6      TA1   9\n",
      "7      BCK   9\n",
      "8      PT1   8\n",
      "9      ME1   8\n",
      "10     SCP   7\n",
      "11     FA3   7\n",
      "12     ME3   7\n",
      "13     NA1   5\n",
      "14     NA2   3\n",
      "15     PT3   2\n",
      "Number of rows: 16\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample 18: Number of Employees by Jobcode\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create frequency table\n",
    "freq_table = df5['JOBCODE'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "freq_table.columns = ['JOBCODE', '#']  \n",
    "\n",
    "# Formatting (adjust as needed, depending on the desired interpretation of 'format=3.0' in SAS)\n",
    "freq_table['#'] = freq_table['#'].astype(int)  # Example: Display counts as integers\n",
    "\n",
    "print('Libname Sample 18: Number of Employees by Jobcode')\n",
    "print(freq_table)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", freq_table.shape[0])\n",
    "print(\"Number of columns:\", freq_table.shape[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 19        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 19: SAMAT5 After Appending SAMDAT6';\n",
    "\n",
    "proc append base=samples.SAMDAT5\n",
    "            data=samples.SAMDAT6;\n",
    "run;\n",
    "\n",
    "proc print data=samples.SAMDAT5;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Appends Datasets: It appends the 'SAMDAT6' dataset onto the end of the 'SAMDAT5' dataset. Both datasets are located in the 'samples' library. Importantly, this modifies the 'SAMDAT5' dataset in-place.\n",
    "Prints Results: It prints the contents of the modified 'SAMDAT5' dataset to show the results of the append operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8500\\1457792929.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Append DataFrames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Print the modified df5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Libname Sample 19: SAMAT5 After Appending SAMDAT6'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6293\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6294\u001b[0m         ):\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6296\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Sample 19: SAMDAT5 After Appending SAMDAT6\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Append DataFrames \n",
    "df5 = df5.append(df6, ignore_index=True) \n",
    "\n",
    "# Print the modified df5\n",
    "print('Libname Sample 19: SAMAT5 After Appending SAMDAT6')\n",
    "print(df5)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df5.shape[0])\n",
    "print(\"Number of columns:\", df5.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 19: SAMAT5 After Appending SAMDAT6\n",
      "    IDNUM SEX JOBCODE   SALARY      BIRTH      HIRED\n",
      "0    1009   M     TA1  28880.0 1959-03-02 1992-03-26\n",
      "1    1017   M     TA3  40858.0 1957-12-28 1981-10-16\n",
      "2    1036   F     TA3  39392.0 1965-05-19 1984-10-23\n",
      "3    1037   F     TA1  28558.0 1964-04-10 1992-09-13\n",
      "4    1038   F     TA1  26533.0 1969-11-09 1991-11-23\n",
      "..    ...  ..     ...      ...        ...        ...\n",
      "155  1369   M     TA3  36598.0 1961-12-28 1987-03-13\n",
      "156  1447   F     FA1  22123.0 1972-08-07 1992-10-29\n",
      "157  1561   M     TA3  36514.0 1963-11-30 1987-10-07\n",
      "158  1639   F     TA3  42260.0 1957-06-26 1984-01-28\n",
      "159  1998   M     SCP  23100.0 1970-09-10 1992-11-02\n",
      "\n",
      "[160 rows x 6 columns]\n",
      "Number of rows: 160\n",
      "Number of columns: 6\n"
     ]
    }
   ],
   "source": [
    "# Sample 19: SAMDAT5 After Appending SAMDAT6\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate DataFrames to create a new one\n",
    "df_appended = pd.concat([df5, df6], ignore_index=True)\n",
    "\n",
    "# Print the appended DataFrame\n",
    "print('Libname Sample 19: SAMAT5 After Appending SAMDAT6')\n",
    "print(df_appended)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_appended.shape[0])\n",
    "print(\"Number of columns:\", df_appended.shape[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 20        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 20: Invoice Frequency by Country';\n",
    "\n",
    "proc freq data=samples.SAMDAT9 (keep=INVNUM COUNTRY);\n",
    "  tables COUNTRY;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code aims to:\n",
    "\n",
    "Generate Frequency Table: Create a frequency table that counts the occurrences of each unique 'COUNTRY' value within the 'SAMDAT9' dataset (located in the 'samples' library).\n",
    "Filter Columns: Keep only the 'INVNUM' and 'COUNTRY' columns in the data for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 20: Invoice Frequency by Country\n",
      "     COUNTRY  COUNT\n",
      "0  Argentina      2\n",
      "1  Australia      1\n",
      "2     Brazil      4\n",
      "3        USA     10\n",
      "Number of rows: 4\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample 20: Invoice Frequency by Country\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create frequency table\n",
    "freq_table = df9[['INVNUM', 'COUNTRY']].groupby('COUNTRY')['INVNUM'].count()\n",
    "freq_table = freq_table.reset_index(name='COUNT')  # Convert to a DataFrame\n",
    "\n",
    "print('Libname Sample 20: Invoice Frequency by Country')\n",
    "print(freq_table)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", freq_table.shape[0])\n",
    "print(\"Number of columns:\", freq_table.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 21        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 21: High Bills--Not Paid';\n",
    "\n",
    "proc sql;\n",
    "  create view work.allinv as\n",
    "  select PAIDON, BILLEDON, INVNUM, AMTINUS, BILLEDTO\n",
    "    from samples.SAMDAT9 (obs=5);\n",
    "quit;\n",
    "\n",
    "data work.notpaid(keep=INVNUM BILLEDTO AMTINUS BILLEDON);\n",
    "\n",
    "  set work.allinv;\n",
    "  where PAIDON is missing and AMTINUS>=300000.00;\n",
    "run;\n",
    "\n",
    "proc print data=work.notpaid label;\n",
    "  format AMTINUS dollar20.2;\n",
    "  label  AMTINUS=amountinus\n",
    "         BILLEDON=billedon\n",
    "         INVNUM=invoicenum\n",
    "         BILLEDTO=billedto;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Creates a View: Defines a view named 'allinv' in the 'work' library. This view includes specific columns (PAIDON, BILLEDON, INVNUM, AMTINUS, and BILLEDTO) from the first five observations (obs=5) of the 'SAMDAT9' dataset (located in the 'samples' library).\n",
    "Filters Data: Creates a new dataset named 'notpaid' in the 'work' library and keeps specific columns. It filters data from the 'allinv' view, selecting rows where:\n",
    "'PAIDON' is missing.\n",
    "'AMTINUS' is greater than or equal to 300000.00.\n",
    "Prints Results: Prints the 'notpaid' dataset with:\n",
    "Custom formatting for the 'AMTINUS' column.\n",
    "Custom labels for the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libname Sample 21: High Bills -- Not Paid\n",
      "   invoicenum  billedto      amountinus   billedon\n",
      "1     11271.0  18543489  $11,063,836.00 1998-10-05\n",
      "Number of rows: 1\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Sample 21: High Bills -- Not Paid\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Simulate the view with filtering for obs=5\n",
    "df_allinv = df9.head(5)[['PAIDON', 'BILLEDON', 'INVNUM', 'AMTINUS', 'BILLEDTO']]\n",
    "\n",
    "# Filter for unpaid invoices with high amounts \n",
    "df_notpaid = df_allinv[(df_allinv['PAIDON'].isnull()) & (df_allinv['AMTINUS'] >= 300000)]\n",
    "df_notpaid = df_notpaid[['INVNUM', 'BILLEDTO', 'AMTINUS', 'BILLEDON']] # Keep specific columns\n",
    "\n",
    "# Format and rename columns\n",
    "df_notpaid['AMTINUS'] = df_notpaid['AMTINUS'].apply('${:,.2f}'.format)\n",
    "df_notpaid.rename(columns={'AMTINUS': 'amountinus', \n",
    "                           'BILLEDON': 'billedon',\n",
    "                           'INVNUM': 'invoicenum',\n",
    "                           'BILLEDTO': 'billedto'}, inplace=True)\n",
    "\n",
    "print('Libname Sample 21: High Bills -- Not Paid')\n",
    "print(df_notpaid)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_notpaid.shape[0])\n",
    "print(\"Number of columns:\", df_notpaid.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*==========================*/\n",
    " /* LIBNAME Sample 22        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 22: Interns Who Are Family Members of Employees';\n",
    "\n",
    "\n",
    "proc sql;\n",
    "  create view emp_csr as\n",
    "  select * from samples.SAMDAT10\n",
    "    where dept in ('CSR010', 'CSR011', 'CSR004');\n",
    "\n",
    "  select samdat13.LASTNAME, samdat13.FIRSTNAM, samdat13.EMPID,\n",
    "         samdat13.FAMILYID, samdat13.GENDER,\n",
    "         samdat13.DEPT, samdat13.HIREDATE\n",
    "    from emp_csr, samples.samdat13\n",
    "    where emp_csr.EMPID=samdat13.FAMILYID;\n",
    "quit;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Intent Summary\n",
    "\n",
    "This SAS code does the following:\n",
    "\n",
    "Creates a View: Defines a view named 'emp_csr' in the 'work' library. This view includes all columns from the 'SAMDAT10' dataset, filtered to include only employees in specific departments ('CSR010', 'CSR011', 'CSR004').\n",
    "Joins Datasets and Filters:\n",
    "Joins the 'emp_csr' view with the 'SAMDAT13' dataset based on the condition that the 'EMPID' in 'emp_csr' matches the 'FAMILYID' in 'SAMDAT13'. This likely aims to identify employees in customer service related departments ('emp_csr') who have family relationships indicated in 'SAMDAT13'.\n",
    "Selects specific columns for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['LASTNAME', 'Gender', 'DEPT', 'HIREDATE'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m df_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_emp_csr, df13, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMPID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Select columns\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df_result \u001b[38;5;241m=\u001b[39m \u001b[43mdf_result\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLASTNAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFIRSTNAM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEMPID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFAMILYID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDEPT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHIREDATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLibname Sample 22: Interns Who Are Family Members of Employees\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_result)\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['LASTNAME', 'Gender', 'DEPT', 'HIREDATE'] not in index\""
     ]
    }
   ],
   "source": [
    "# Sample 22: Interns Who Are Family Members of Employees \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create the 'emp_csr' view\n",
    "df_emp_csr = df10[df10['DEPT'].isin(['CSR010', 'CSR011', 'CSR004'])]\n",
    "\n",
    "# Join and filter\n",
    "df_result = pd.merge(df_emp_csr, df13, on='EMPID', how='inner')\n",
    "\n",
    "# Select columns\n",
    "df_result = df_result[['LASTNAME', 'FIRSTNAM', 'EMPID', 'FAMILYID', 'Gender', 'DEPT', 'HIREDATE']]\n",
    "\n",
    "print('Libname Sample 22: Interns Who Are Family Members of Employees')\n",
    "print(df_result)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_result.shape[0])\n",
    "print(\"Number of columns:\", df_result.shape[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['FAMILYID'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m df_emp_csr \u001b[38;5;241m=\u001b[39m df10[df10[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEPT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSR010\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSR011\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSR004\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Join and filter (assuming you want to keep the FAMILYID from df_emp_csr)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\u001b[43mdf_emp_csr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEMPID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFAMILYID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, df13, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAMILYID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Select columns (ensure these names actually exist in your df_result)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m df_result \u001b[38;5;241m=\u001b[39m df_result[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLASTNAME\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFIRSTNAM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMPID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAMILYID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGENDER\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEPT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHIREDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['FAMILYID'] not in index\""
     ]
    }
   ],
   "source": [
    "# Sample 22: Interns Who Are Family Members of Employees \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create the 'emp_csr' view\n",
    "df_emp_csr = df10[df10['DEPT'].isin(['CSR010', 'CSR011', 'CSR004'])]\n",
    "\n",
    "# Join and filter (assuming you want to keep the FAMILYID from df_emp_csr)\n",
    "df_result = pd.merge(df_emp_csr[['EMPID', 'FAMILYID']], df13, on='FAMILYID', how='inner')\n",
    "\n",
    "# Select columns (ensure these names actually exist in your df_result)\n",
    "df_result = df_result[['LASTNAME', 'FIRSTNAM', 'EMPID', 'FAMILYID', 'GENDER', 'DEPT', 'HIREDATE']]\n",
    "\n",
    "print('Libname Sample 22: Interns Who Are Family Members of Employees')\n",
    "print(df_result)\n",
    "\n",
    "# Output dimensions\n",
    "print(\"Number of rows:\", df_result.shape[0])\n",
    "print(\"Number of columns:\", df_result.shape[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
